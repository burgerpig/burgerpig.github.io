# M6 Hypothesis Testing

![image-20230725204350400](./assets/image-20230725204350400.png)

## R6.1 Steps and Basic Concepts

- Hypothesis testing is an act in statistics whereby an analyst tests an assumption regarding a **population parameter** å¯¹äºæ€»ä½“å‚æ•°çš„å‡è®¾çš„æ£€éªŒ

  - åŸºç¡€ï¼šè®¤ä¸ºå°æ¦‚ç‡äº‹ä»¶ä¸ä¼šå‘ç”Ÿ

- **Interval Estimation** and **Hypothesis testing**

  - Estimation
    - Addresses the questions such as "what is this parameter's value"

  - Hypothesis testing
    - Addresses the questions such as "is the value of the parameter equal to a specific value"
  - **Important question**
    - What's the logic between interval estimation and hypothesis testing?
      - å‡è®¾æ£€éªŒåŒ…æ‹¬å•è¾¹å’ŒåŒè¾¹
      - åŒºé—´ä¼°è®¡å°±æ˜¯åŒè¾¹æ£€éªŒ
      - ä¸¤è€…é€»è¾‘ä¸€è‡´ï¼Œå‡è®¾æ£€éªŒæ ¸å¿ƒæ€æƒ³æ˜¯å°æ¦‚ç‡äº‹ä»¶éœ€è¦æ‹’ç»ï¼›åŒºé—´ä¼°è®¡ä¹Ÿæ˜¯ç«™åœ¨æŸä¸ªæ¦‚ç‡çš„è§’åº¦ï¼Œçœ‹åŒºé—´æ˜¯å¦èƒ½è¦†ç›–æ€»ä½“å‚æ•°ã€‚

#### Steps of Hypothesis Testing

1. Stating the **hypothesis:** relation to be tested. å…³äºæ€»ä½“å‚æ•°çš„å‡è®¾.
2. Identifying the appropriate **test statistic** and its **probability distribution**
   - æ„é€ æ£€éªŒç»Ÿè®¡é‡ï¼ˆtest statistic, ä¸€ä¸ªéšæœºå˜é‡ï¼‰
3. specifying the **significance level**
4. stating the **decision rule**
5. collecting the data and calculating the **value of test statistic**
   - è®¡ç®—æ£€éªŒç»Ÿè®¡é‡çš„ä¸€æ¬¡è®¡ç®—çš„å…·ä½“å–å€¼
6. making the **statistical decision**
   - æ ¹æ®4ï¼‰çš„è§„åˆ™ï¼Œåšå‡ºåˆ¤æ–­ã€‚ï¼ˆæ‹’ç»å°æ¦‚ç‡äº‹ä»¶ï¼‰
7. making the **economic or investment decision**

#### Null hypothesis and Alternative Hypothesis

- **Null hypothesis($H_0$):** Hypothesis to be tested åŸå‡è®¾
- **Alternative Hypothesis($H_a$)å¤‡æ‹©å‡è®¾**: The opposite side of null hypothesisï¼ˆå’ŒH0äº’æ–¥ä¸”äº’è¡¥ï¼‰
- Example1:
  - H0: average BMI for BMs is euqal to 30
  - Ha: average BMI for BMs is not equal to 30
- Example 2:
  - H0: average BMI for BM is less than or equal to 30
  - Ha: average BMI for BMs is more than 30
- **Important Conclusion**
  - The "=" sign will be showed in null hypothesis
    - ç­‰å·ä¸€å®šè¦å‡ºç°åœ¨åŸå‡è®¾ã€‚è¦æ ¹æ®åŸå‡è®¾æ„å»ºæ£€éªŒç»Ÿè®¡é‡
  - The only decision we can make is whether to reject the original hypothesis or not
    - é’ˆå¯¹åŸå‡è®¾è¿›è¡Œæ£€éªŒï¼ˆä¸åˆ¤æ–­Haçš„æ­£ç¡®æ€§ï¼Œåªæ£€éªŒæ˜¯å¦æ‹’ç»H0ï¼‰
    - æ³¨æ„ï¼Œ"æ¥å—H0"çš„è¯´æ³•ä¹Ÿæ˜¯é”™è¯¯ã€‚è¿™é‡Œåªæ£€éªŒæ˜¯å¦æ‹’ç»H0
    - **ä¸è¦è½»æ˜“è¯´æ¥å—**ï¼š
      - æ‹’ç»H0ï¼Œä¸ç­‰äºæ¥å—Ha
      - ä¸æ‹’ç»H0ï¼Œä¸ç­‰äºæ¥å—H0
      - è¿™æ˜¯å› ä¸ºæˆ‘ä»¬ç«™åœ¨æ£€éªŒç»Ÿè®¡çš„è§’åº¦çœ‹é—®é¢˜ã€‚å¦‚æœå‘ç”Ÿäº†å°æ¦‚ç‡äº‹ä»¶ï¼Œæˆ‘ä»¬è¦æ‹’ç»H0ã€‚ä½†æ˜¯å¦‚æœå‘ç”Ÿäº†å¤§æ¦‚ç‡äº‹ä»¶ï¼Œä¸èƒ½è¯´æ˜H0ä¸€å®šæ˜¯å¯¹çš„ã€‚ï¼ˆè°¨æ…æ€§ï¼Œè¯ä¼ªåªéœ€è¦ä¸¾å‡ºä¸€ä¸ªåä¾‹ï¼ˆå°æ¦‚ç‡äº‹ä»¶ï¼‰ï¼Œç„¶è€Œè¯æ˜è¦ä¸¥æ ¼çš„é€»è¾‘ï¼Œä¸èƒ½å•çº¯é æ£€éªŒç»Ÿè®¡é‡ä¸æ–­æ£€éªŒã€‚æ¯”å¦‚ï¼šå¯¹äº†ä¸€ä¸‡æ¬¡ï¼Œä¸èƒ½è¯´æ˜ä¸‹ä¸€æ¬¡è¿˜ä¼šå¯¹ï¼Œä½†æ˜¯åªè¦é”™äº†ä¸€æ¬¡ï¼Œå°±è¯´æ˜ä¸‹ä¸€æ¬¡è‚¯å®šè¿˜ä¼šé”™ã€‚ï¼‰
  - Null hypothesis is that analyst wants to reject
    - æƒ³è¦æ‹’ç»çš„å‡è®¾ï¼Œä¸€èˆ¬æ”¾åœ¨H0
    - å‡è®¾æ£€éªŒçš„ç›®çš„ï¼Œå°±æ˜¯æ‹’ç»æ‰ä¸€ä¸ªä¸æƒ³è¦çš„å‡è®¾ã€‚å› ä¸ºè¯ä¼ªæ€»æ¯”è¯æ˜å®¹æ˜“ã€‚
      - å¦‚æœæŠŠä¸€ä¸ªæƒ³è¦çš„å‡è®¾æ”¾åœ¨äº†H0ï¼Œæœ€åçš„æ£€éªŒç»Ÿè®¡é‡å‡ºæ¥äº†ä¸æ‹’ç»H0. å¹¶ä¸èƒ½è¯´æ˜æ¥å—H0.
      - è€ŒæŠŠä¸€ä¸ªä¸æƒ³è¦çš„å‡è®¾æ”¾åœ¨H0ï¼Œæœ€åæ£€éªŒç»Ÿè®¡é‡è®¡ç®—å‡ºæ‹’ç»H0ï¼Œå¯¹ç ”ç©¶çš„é—®é¢˜å°±äº§ç”Ÿäº†è¿›å±•ã€‚

#### Two tailed Test åŒè¾¹æ£€éªŒ vs. One-Tailed Test

- **Two tailed test**
  - Used to test if a population parameter is different from a specified value
  - $H_0: \theta=\theta\ vs.\ H_a:\theta \ne \theta_0$
  - è¿™ç§æƒ…å†µï¼Œæ‹’ç»åŸŸï¼ˆæ­£æ€åˆ†å¸ƒçš„ä¸¤ä¸ªå°¾å·´ï¼‰æ˜¯åŒè¾¹çš„ã€‚
- **One-tailed test**
  - Used to test if a parameter is above or below a specified value
  - $H_0: \theta\le\theta_0\ vs.\ H_a:\theta \gt \theta_0$
    - æ‹’ç»åŸŸåœ¨å³è¾¹
  - $H_0: \theta\ge\theta_0\ vs.\ H_a:\theta \lt \theta_0$
    - æ‹’ç»åŸŸåœ¨å·¦è¾¹
  - æ‹’ç»åŸŸåœ¨åŸå‡è®¾çš„å¦å¤–ä¸€è¾¹
- ä¸Šé¢$\theta$æ˜¯æ€»ä½“å‚æ•°ï¼Œ$\theta_0$æ˜¯å‡è®¾çš„æŸä¸ªå€¼ï¼ˆå¸¸æ•°ï¼‰ã€‚æ‰€ä»¥å‡è®¾éƒ½æ˜¯å¯¹å¸¸æ•°è¿›è¡Œå‡è®¾

#### Critical Concepts

- **Test statistic's value** æ£€éªŒç»Ÿè®¡é‡çš„value
  - A quantity calculated based on a sample
  - æ£€éªŒç»Ÿè®¡é‡æ˜¯éšæœºå˜é‡ï¼ŒåŸºäºæŠ½æ ·è®¡ç®—å¾—åˆ°test statistic's value
- **Significance level($\alpha$)** æ˜¾è‘—æ€§æ°´å¹³
  - The level of significance reflects how much sample evidence we require to reject the null
  - å°æ¦‚ç‡äº‹ä»¶å¯¹åº”çš„é¢ç§¯
- **Critical value(rejection point)** ä¸´ç•Œå€¼
  - A value with which the computed test statistic is compared to decide whether to reject or not reject the null hypothesis
  - å°±æ˜¯$\alpha$å¯¹åº”çš„ç½®ä¿¡åŒºé—´çš„è¾¹ç•Œreliability factor($z_{\alpha/2}$)
  - ä¸´ç•Œå€¼å’Œ$\alpha$ä¸€ä¸€å¯¹åº”
  - æ‹’ç»åŸŸï¼šæ‹’ç»åŸå‡è®¾çš„åŒºåŸŸï¼ˆæ˜¾è‘—æ€§æ°´å¹³å¯¹åº”çš„å°¾å·´ï¼‰
  - æ£€éªŒç»Ÿè®¡é‡valueå’Œcritical valueåšæ¯”è¾ƒï¼Œè¶…è¿‡critical valueæ—¶ï¼ˆvalueè½åœ¨å°¾å·´ï¼‰ï¼Œè®¤ä¸ºæ—¶å°æ¦‚ç‡äº‹ä»¶ï¼Œæ‹’ç»H0ã€‚
- **p-value**
  - The smallest level of significance at which the null hypothesis can be rejected
  - p-valueå°±æ˜¯è®¡ç®—å‡ºtest statistic valueæ—¶ï¼Œç§¯åˆ†å¯¹åº”æ£€éªŒç»Ÿè®¡é‡åˆ†å¸ƒçš„å³ä¾§å¾—åˆ°çš„å€¼ã€‚ï¼ˆtest statistic valueå¯¹åº”çš„å°¾å·´çš„é¢ç§¯ï¼‰ï¼Œ
    - ä¸€èˆ¬p valueå’Œ$\alpha$ä½œæ¯”è¾ƒï¼š
      - å½“ä¸€ä¾§çš„å°¾å·´é¢ç§¯æ˜¯$\alpha/2$æ—¶ï¼Œæ‰€ä»¥ç®—p-valueæ—¶éœ€è¦å•è¾¹å°¾å·´é¢ç§¯ä¹˜ä»¥2
      - å½“ä¸€ä¾§å°¾å·´é¢ç§¯å°±æ˜¯$\alpha$æ—¶ï¼Œp-valueå°±æ˜¯å•è¾¹å°¾å·´çš„é¢ç§¯ï¼ˆä¸‹é¢çš„one-tailed testï¼‰
  - åˆ¤æ–­æ˜¯å¦æ‹’ç»å‡è®¾æ—¶ï¼Œçœ‹p-value æ˜¯å¦å°äºæŸä¸ªæ˜¾è‘—æ€§æ°´å¹³$\alpha$ 
  - p-valueå’Œsignificance levelæ¯”è¾ƒï¼›test statistic valueå’Œcritical valueæ¯”è¾ƒ

#### Decision rules

- If test statistic is outside the range of critical value(test statistic >= upper critical value, or test static value < = lower critical value), rejuct the null hypothesis
- If the p-value is less or euqal to the level of significant ($\alpha$), reject the null hypothesis

#### Example: Two-tailed test of population mean

- $H_0:\mu = 0\ v.s.\ H_a:\mu\ne 0$

![image-20230725214330614](./assets/image-20230725214330614.png)

- ä¸Šå›¾ä¸­ï¼Œp-value:2.14å’Œ5æ¯”è¾ƒï¼›ä¹Ÿå¯ä»¥1.07å’Œ2.5æ¯”è¾ƒ

#### Example: One-tailed test of population mean

- $H_0: \mu \le 0\ v.s.\ H_a:\mu>0$

<img src="./assets/image-20230725214556109.png" alt="image-20230725214556109" style="zoom: 50%;" />

- æ³¨æ„ï¼šä¸Šé¢5%å¯¹åº”çš„æ˜¯1.645çš„critical valueã€‚ï¼ˆå¯¹åº”90%ç½®ä¿¡æ°´å¹³ï¼‰
- å¦å¤–ï¼Œå¦‚æœè¿™æ—¶å€™ç®—åˆ°çš„test statistic valueå¯¹åº”çš„å³è¾¹å°¾å·´é¢ç§¯æ˜¯3ï¼Œé‚£3å’Œ5å¯¹æ¯”ï¼Œp-valueå°±æ˜¯3ï¼Œä¸éœ€è¦ä¹˜ä»¥2.

#### Multiple Testing Problem - Example å¤šæ¬¡æµ‹è¯•

- éœ€è¦ä¼°è®¡æ€»ä½“å‚æ•°ï¼Œä»æ€»ä½“ä¸­é‡‡æ ·samplingï¼Œå¾—åˆ°ä¸€ä¸ªæ ·æœ¬ï¼ˆæ ·æœ¬å®¹é‡nï¼‰ï¼Œè®¡ç®—å¾—åˆ°æ£€éªŒç»Ÿè®¡é‡è¿›è¡Œæ£€éªŒï¼›å¦‚æœè¦ç»§ç»­resamplingï¼Œä¼šè®¡ç®—å¾—åˆ°å¤šä¸ªæ£€éªŒç»Ÿè®¡é‡ã€‚è¿™é‡Œçš„multiple testing problemè€ƒè™‘çš„å°±æ˜¯resamplingä¸‹å¦‚ä½•è¿›è¡Œå‡è®¾æ£€éªŒã€‚
  - æŠ½æ ·æ¬¡æ•°ï¼šnumber of samples
  - æ ·æœ¬å®¹é‡: sample size

![image-20230725215057682](./assets/image-20230725215057682.png)

![image-20230725215105595](./assets/image-20230725215105595.png)

- corrected p-valueè°ƒæ•´åçš„p-value
- false discovery approachï¼šåŸæœ‰çš„p-valueæ‹’ç»ï¼Œä½†æ˜¯corrected p-valueæ²¡æœ‰æ‹’ç»ã€‚è¿™æ˜¯false discovery
  - åŸæ¥çš„å‡è®¾æ£€éªŒæ–¹æ³•:$pvalue < \alpha$
  - correctedæ–¹æ³•ï¼š$pvalue \lt \alpha\frac{i}{k}$
  - iæ˜¯på€¼çš„æ’åºï¼Œkæ˜¯æ ·æœ¬é‡ï¼ˆæŠ½æ ·æ¬¡æ•°ï¼‰
  - æ˜¾ç„¶correctedåï¼Œå¯¹pvalueçš„è¦æ±‚æ›´ä¸ºä¸¥æ ¼ï¼ˆè¦æ±‚æ›´å°ï¼‰

![image-20230725215111584](./assets/image-20230725215111584.png)

## R6.2 Type I error and Type II error

- **Type I Error**
  - **rejecting null hypothesis when it is true**
    - åŸå‡è®¾æ­£ç¡®ï¼Œå´æ‹’ç»äº†åŸå‡è®¾ï¼ˆæ‹’çœŸï¼‰
  - **P(Type I error) = significance level $\alpha$ **

- **Type II error**
  - **failing the reject the null hypothesis when it is false**
    - åŸå‡è®¾é”™è¯¯ï¼Œå´æ²¡æœ‰æ‹’ç»åŸå‡è®¾ï¼ˆçº³ä¼ªï¼‰
  - P(Type II error) = $\beta$
  - **Power of test**: rejecting the null hypothesis when it is false
    - Power of test = 1 - P(type II error) = $1-\beta$
    - power: åŠ¿

#### Question

- Since both type I and Type II are errors, can they be avoided at the same time?
  - ä¸å¯ä»¥åŒæ—¶é™ä½Type I and Type IIã€‚é™ä½Type Iä»£è¡¨æ›´è°¨æ…ï¼Œå°½å¯èƒ½æ‹’ç»ï¼Œæ­¤æ—¶æé«˜äº†Type IIï¼›é™ä½Type IIä»£è¡¨å°½å¯èƒ½æ¥å—ï¼Œé¿å…è¯¯åˆ¤ï¼Œæ­¤æ—¶æé«˜äº†Type Iã€‚æœ‰ç‚¹åƒprecision å’Œ recallçš„å…³ç³»
- How should we improve the plan in order to avoid them as much as possibleï¼Ÿ
  - æé«˜æ ·æœ¬é‡ï¼Œå¹¶ä¸”ç”¨éšæœºæŠ½æ ·
- Which error is more fatal and whyï¼Ÿ
  - Type I error is more fatalã€‚ä¸€ç±»é”™è¯¯æ›´é‡è¦ã€‚
  - å‡è®¾æ£€éªŒæœ¬èº«çš„ç›®çš„å°±æ˜¯è¯ä¼ªï¼Œæ‰¾åˆ°ä¸€ä¸ªå°æ¦‚ç‡äº‹ä»¶ï¼Œæ‹’ç»æ‰H0ã€‚
- P(type I error) + P(type II error) $=?$ 100%ã€‚âŒï¼Œä¸ç­‰äº100%ï¼ã€‚
  - P(æ²¡æœ‰æ‹’ç»ï½œåŸå‡è®¾æ­£ç¡®) = 1 - alpha
  - P(æ‹’ç»ï½œåŸå‡è®¾æ­£ç¡®) = alpha = P(type I error)
  - P(æ²¡æœ‰æ‹’ç»ï½œåŸå‡è®¾é”™è¯¯) = beta = P(type II error)
  - P(æ‹’ç»ï½œåŸå‡è®¾é”™è¯¯) = 1 - beta = power of test
- H0 Trueè¡¨ç¤ºåŸå‡è®¾æ˜¯æ­£ç¡®çš„æƒ…å†µä¸‹ï¼ŒH0 Falseè¡¨ç¤ºåŸå‡è®¾é”™è¯¯ï¼š

| Decision         | H0 True                          | H0 False                                          |
| ---------------- | -------------------------------- | ------------------------------------------------- |
| Do not reject H0 | correct decision(P = $1-\alpha$) | Type II error(P = $\beta$)                        |
| Reject H0        | Type I error(P = $\alpha$)       | Correct decision(P = power of test=(1 - $\beta$)) |

#### Type I Error and False Positive Result

- A **type I Error** is the risk of **rejection of a true null hypothesis**

  - false postive resultå°±æ˜¯çŠ¯äº†ç¬¬ä¸€ç±»é”™è¯¯

- Another way of phrasing this is that it is a **false positive result**

  - That is, the null is rejected(the positive), yet the null is true(hence, a false positive)

  - è¿™ä¸å°±æ˜¯æ··æ·†çŸ©é˜µ(confusion matrix)

  - |                     | actual Positive | actual Negative |
    | ------------------- | --------------- | --------------- |
    | prediction postive  | TP              | FP              |
    | prediction negative | FN              | TN              |

    è¿™é‡Œé¢„æµ‹positiveï¼Œå°±æ˜¯æ‹’ç»nullï¼Œé‚£ä¹ˆå¦‚æœåŸå‡è®¾æ­£ç¡®ï¼Œæ„æ€æ˜¯actual negativeã€‚é‚£å°±æ˜¯prediction postive yet actual negativeï¼Œå°±æ˜¯FP, false positive.

#### Make A decision

- Make a **statistical decision**
  - åšå®Œç»Ÿè®¡å†³ç­–ï¼Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µ(ç¨æ”¶ã€äº¤æ˜“è´¹ç”¨ã€é£é™©ç­‰)è€ƒè™‘ç»æµå†³ç­–
- Make an **economic decision**
  - The economic or investment decision takes into consideration not only the statistical decision but also all pertinent economic issues
- We frequently find that slight differences between a variable and its hypothesized value are statistically significant but not economically meaningful
  - The standard error decreases as the sample size n increases, so that we can reject the null and find that it provides a statistically significant return
  - The reuslts may not be economically significant when we account for **transaction costs, taxes, and risk**

![image-20230726075423623](./assets/image-20230726075423623.png)

## R6.3 Hypothesis Testing

### 6.3.1 Single Mean

1) åŒºé—´ä¼°è®¡æ˜¯æœªçŸ¥$\mu$ï¼ŒæŠŠåŒºé—´ç®—å‡ºæ¥ï¼Œç„¶åå¾—å‡ºåŒºé—´è¦†ç›–æ€»ä½“å‚æ•°çš„æ¦‚ç‡ï¼›å‡è®¾æ£€éªŒæ˜¯çš„å‡è®¾çš„$\mu_0$ï¼Œä»£å…¥åˆ°$\bar X-\mu/\sigma/\sqrt n$ä¸­ï¼Œå’Œåˆ†ä½æ•°åšæ¯”è¾ƒã€‚
2) åŒè¾¹ã€å•è¾¹ã€‚å•è¾¹çš„å‡è®¾æ˜¯H0: mu = mu0; åŒè¾¹çš„å‡è®¾H0: mu <= mu0ã€‚æ³¨æ„ï¼ŒH0æ˜¯æœŸæœ›æ‹’ç»æ‰çš„å‡è®¾ï¼Œä¹Ÿå°±æ˜¯æŠŠæœ€ä¸å¯èƒ½çš„å‡è®¾æ”¾åœ¨H0ã€‚æ¯”å¦‚H0: å¹³å‡èº«é«˜<=160ã€‚
3) æœªçŸ¥sigmaï¼Œä½¿ç”¨æ ·æœ¬æ ‡å‡†å·®ä»£æ›¿ï¼Œè¿™æ—¶å€™ä¸­å¿ƒæé™å®šç†å˜ä¸ºtåˆ†å¸ƒã€‚

- Test statistic for hypothesis test of population mean with **known variance**

- $$
  z = \frac{\bar X - \mu_0}{\sigma/\sqrt n}
  $$

  - ğŸ– is ğŸ–-statistic

  - $\bar X$ is sample mean

  - $\mu_0$ is hypothesis value of the population mean

  - $\sigma$ is population standard deviation

- Test statistic for hypothesis tests of population mean with **unknown variance**

- $$
  t_{n-1}=\frac{\bar X-\mu_0}{s/\sqrt n}\\
  z=\frac{\bar X-\mu_0}{s/\sqrt n}\ \ \ when\ sample\ size\ is\ large(\ge30)
  $$

  - $t_{n-1}$ is the t-statistic with n-1 degrees of freedom(n is the sample size)
  - $\bar X$ is the sample mean
  - $\mu_0$ is the hypothesized value of the population mean
  - s is the sample standard deviation

![image-20230726083455883](./assets/image-20230726083455883.png)

![image-20230726083505393](./assets/image-20230726083505393.png)

![image-20230726083614212](./assets/image-20230726083614212.png)

- believe >1%ï¼Œæ‰€ä»¥æŠŠé¢„æœŸæ‹’ç»çš„<=1%æ”¾åœ¨H0

![image-20230726083624008](./assets/image-20230726083624008.png)

![image-20230726083854593](./assets/image-20230726083854593.png)

### 6.3.2 Other situations

#### Test Concerning Differences Between means with independent samples

- independentçš„ä¸¤ä¸ªå˜é‡ï¼Œå¯¹æ¯”å‡å€¼
- Let $\mu_1$ and $\mu_2$ represent, respectively, the population means of the first and second populations
- Two-sided:
  - $H_0:\mu_1-\mu_2=d$
  - $H_a:\mu_1-\mu_2\ne d$
- One-sided:
  - $H_0:\mu_1-\mu_2\le (\ge)d$
  - $H_a:\mu_1-\mu_2\gt(\lt) d$

- When we can assume that the two populations are **normally distributed** and that the **unknown population variances are equal**, we use a **t-distributed test statistic** based on independent random samples.
  - **ä¸‰ä¸ªæ¡ä»¶ï¼šç‹¬ç«‹(independent)ï¼Œ(normally distributed)æ­£å¤ªåˆ†å¸ƒï¼Œæ€»ä½“æœªçŸ¥æ–¹å·®ç›¸ç­‰**

$$
t=\frac{(\bar X_1-\bar X_2)-(\mu_1-\mu_2)}{\sqrt{\frac{s_p^2}{n_1}+\frac{s_p^2}{n_2}}}
$$

where,
$$
s_p^2=\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}
$$

- The number of **degrees of freedom** for this t-distribution test statistic is $n_1+n_2-2$

####  Test Concerning Differences between means with dependent samples

- When we want to conduct tests on two means based on samples that we believe are **dependent**, we use the test of **the mean of the differences**(a.k.a. **paired comparisons test**)
  - Suppose we have observations for the random variables $X_A$ and $X_B$, and $d_i=x_{A_i}-x_{B_i}$
- Two-sided:
  - $H_0:\mu_d=\mu_{d_0}$
  - $H_a:\mu_d\ne\mu_{d_0}$
- One-sided:
  - $H_0:\mu_d\ge(\le)\mu_{d_0}$
  - $H_a:\mu_d<(>)\mu_{d_0}$
- When we have data consisting of paired observations from samples generated by **normally distributed** populations with **unknown variances,** the **t-distributed test statistic** is
  - ä¸‰ä¸ªè¦ç‚¹ï¼š1) dependent, 2) normally distributed; 3) unknown variances

$$
t=\frac{\bar d-\mu_{d_0}}{s_{\bar d}}
$$

where
$$
\bar d=\frac{1}{n}\sum_{i=1}^{n}d_i\\
s_{\bar d}=\frac{s_d}{\sqrt n}
$$

- **The number of degrees of freedom for this t-distributed test statistic is $n-1$**

#### Testing concerning Tests of variances

- Two-sided: 
  - $H_0:\sigma^2=\sigma_0^2$
  - $H_a:\sigma^2\ne\sigma_0^2$

- One-sided:
  - $H_0:\sigma^2\le(\ge)\sigma_0^2$
  - $H_a:\sigma^2>(<)\sigma_0^2$

- In tests concerning the variance of a **single normally distributed population**, we make use of a **chi-square test statistic**

$$
\chi^2=\frac{(n-1)s^2}{\sigma_0^2}
$$

- The number of degrees of freeedom for this t-distributed test statistic is **n-1**

#### Test Concerning the equality of two variances

- Suppose we have a hypothesis about the relative values of the variances of **two normally distribute populations** with variances of $\sigma_1^2$ and $\sigma_2^2$
- Two-sided: 
  - $H_0:\sigma_1^2=\sigma_2^2$
  - $H_a:\sigma_1^2\ne\sigma_2^2$

- One-sided:
  - $H_0:\sigma_1^2\le(\ge)\sigma_2^2$
  - $H_a:\sigma_1^2>(<)\sigma_2^2$

- Suppose we have two samples, the first with $n_1$ observations and a sample variance $s_1$, and the second with $n_2$ observations and a sample variance $s_2$
  - The samples are random, **independent** of each other, and generated by **normally distributed population**

$$
F=\frac{s_1^2}{s_2^2}
$$

- ä¸¤ä¸ªæ ·æœ¬æ–¹å·®ç›¸é™¤ï¼Œå¤§çš„æ ·æœ¬æ–¹å·®æ”¾ä¸Šé¢ã€‚
- $df_1=(n_1-1)$ numerator degrees of freedom
- $df_2=(n_2-1)$ denominator degrees of freedom
- è‡ªç”±åº¦æ˜¯(n1-1, n2-1)
- When we rely on tables to arrive at critical values, a convention is to use the larger of the two sample variances in the numberator

![image-20230726215958216](./assets/image-20230726215958216.png)

### 6.3.3 Hypothesis Test for Relationship

#### Tests Concerning Correlation \*\*\*

- In many contexts in investments, we want to assess the strength of the linear relationship betwen two variables, thus we want to evaluate the correlation between them
- Two sided:
  - $H_0: \rho=0$
  - $H_a:\rho\ne 0$
- One-sided:
  - $H_0:\rho\le(\ge)0$
  - $H_a:\rho >(<)0$
- If the two variables are **normally distributed**, we can test to determine whether the null hypothesis($H_0:\rho=0$) should be rejected using the sample correlation, r(æ ·æœ¬ç›¸å…³ç³»æ•°)
  - The formula for the **t-test** is \*\*\*

$$
t=\frac{r\sqrt{n-2}}{\sqrt{1-r^2}}
$$

**The nubmer of degrees of freedom for this t-distributed test statistic is $n-2$**

æœ€åï¼Œå…³äºä¸Šé¢çš„å¼å­ä¸­çš„æ ·æœ¬ç›¸å…³ç³»æ•°rï¼Œä¸‹é¢ç»™å‡ºäº†ä¸¤ç§è®¡ç®—æ–¹æ³•ã€‚

##### Pearson Correlation and Spearman Rank Correlation

- The parametric pariwise correlation coefficient is often referred to as the Pearson correlation, the bivariate correlation, or simply the sample correlation r.
- æ ·æœ¬ç›¸å…³ç³»æ•°ä¸ºï¼š

$$
r=\frac{s_{XY}}{s_Xs_Y}
$$

- ä¸Šé¢çš„æ–¹æ³•æ˜¯Parametric Testsï¼Œä¸‹é¢æ˜¯Nonparametric Tests

- When we believe that the population under consideration meaningfully **departs from normality**(ä¸ç¬¦åˆæ­£æ€åˆ†å¸ƒæ—¶), we can use a test based on the **Spearman rank correlation coefficient** 

$$
r_s=1-\frac{6\sum_{i=1}^{n}d_i^2}{n(n^2-1)}
$$

- dæ˜¯differenceï¼Œä¸¤ä¸ªå˜é‡çš„rank differenceã€‚
- The test of hypothesis for teh Spearman rank correlation depends on whether the sample is small or large(n>30)
  - For small samples, the researcher requries a specialized table of critical values
  - For large samples, we can conduct a **t-test with n-2 degrees of freedom**

![image-20230726221903231](./assets/image-20230726221903231.png)

#### Parametric Tests vs. NonParameteric Tests

##### Parametric tests

- Based on assumptions about population distributions and population parameters. 
  - e.g., t-test, z-test, F-test

##### Non-parametric tests

- Test things other than paramter values
- Applied when:
  - **Data do not meet distributional assumptions**
  - **Data are given in ranks**
  - **The hypothesis we are addressing does not concern a parameter**

#### Test of Independence Using Contingency Table Data

- When faced with **categorical or discrete data**, we **cannot** use the methods that we have dicussed up to this point to test whether the classifications of such data are independent. è¿™é‡Œè¦éªŒè¯çš„æ˜¯åˆ—è”è¡¨çš„ä¸¤ä¸ªç»´åº¦æ˜¯ä¸æ˜¯ç›¸å…³ã€‚
- The frequency table is called **contingency table**
- If we want to test wheter there is a **relationship between the row and column**, we can perform a test of independence using a **non-parametric test statistic** that is **chi-square** distributed

$$
\chi^2=\sum_{i=1}^{m}{\frac{(O_{ij}-E_{ij})^2}{E_{ij}}}
$$

- m = the number of cells in the table

- Oij = the number of observationsï¼ˆäº‹å®ä¸Šçš„åˆ—è”è¡¨çš„æ•°å­—ï¼‰ in each cell of row i and column j(i.e. observed frequency)

- Eij = the expected number of observations in each cell, assuming independence(i.e, expected frequency)
  $$
  E_{ij}=\frac{Total\ row\ i \times Total\ column\ j}{Overal\ total}
  $$

- **The number of degrees of freedom is $(r-1)(c-1)$**

  - rå’Œcæ˜¯æ¨ªçºµåæ ‡çš„ç»´åº¦

- ä¸‹é¢çš„æ¡ˆä¾‹ï¼Œä¸Šé¢çš„è¡¨æ ¼æ˜¯observationï¼Œä¸‹é¢æ˜¯expectation

![image-20230726223238870](./assets/image-20230726223238870.png)

- æ³¨æ„ï¼Œä¸Šé¢çš„ä¾‹å­ä¸­chi-squareæ˜¯å•è¾¹æ£€éªŒã€‚

![image-20230726223244642](./assets/image-20230726223244642.png)































































 
